# ðŸ““ NLP Study Group 

Navigation:

- [Week 1](#Week_1) (_Discussion: February 28, 2020_)
- [Week 2](#Week_2)
- [Week 3](#Week_3)

## **Week 0**

Introductions! 

_What does your current work look like? How do you plan to use NLP?_

--------
## **Week 1** 

1. What is NLP?

- A changing field
- Resources
- Tools
- Python libraries
- Example applications
- Ethics issues

Notebook: [What is NLP?](https://github.com/fastai/course-nlp/blob/master/1-what-is-nlp.ipynb)

Video(s):
- FastAI: [What is NLP?](https://www.youtube.com/watch?v=cce8ntxP_XI&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=2&t=0s)
- [SOTA 2019](https://www.youtube.com/watch?v=YxMW0w7ojek&feature=emb_logo) 


--------
## **Week 2** 

Video(s):
- FastAI: [Topic Modeling with SVD & NMF](https://www.youtube.com/watch?v=tG3pUwmGjsc&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=2)

Notebook: 
- [Topic Modeling with SVD & NMF](https://github.com/fastai/course-nlp/blob/master/2-svd-nmf-topic-modeling.ipynb)

Resources:
- [A Tutorial on PCA (closely related to SVD)](https://arxiv.org/pdf/1404.1100.pdf)
- [SVD article, explaining intuition behind factorizing to 3 different matrices](https://towardsdatascience.com/svd-8c2f72e264f)

## **Week 3**
Video(s):
- FastAI: [Logistic Regression & Naive Bayes](https://www.youtube.com/watch?v=hp2ipC5pW4I&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=4)


Notebook: 
- [Logistic Regression & Naive Bayes](https://github.com/fastai/course-nlp/blob/master/3-logreg-nb-imdb.ipynb)

Resources:
- [Baysean Inference](https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348)

Questions:
- Why is it when you plot the sorted document-term matrix it shows those waves

## **Week 4**
Skipped

## **Week 5**
Video(s):
- FastAI: [Transfer Learning](https://www.youtube.com/watch?v=PNNHaQUQqW8&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=8)

Notebook: 
- [Transfer Learning](https://github.com/fastai/course-nlp/blob/master/3-logreg-nb-imdb.ipynb)


Resources:
- [NLP's ImageNet Movement](https://thegradient.pub/nlp-imagenet/)
- [Cyclical Learning Rates](https://arxiv.org/pdf/1506.01186.pdf)
- [Stop using Word2Vec](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)

Questions:
- When we pick a learning rate off the LR vs. Loss graph, we pick it halway down the descent to the lowest point. Why is that?

## **Other Resources**

### **Books**
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [FastAI Book](https://github.com/fastai/fastbook)

### **Classes** 
- [Linear Algebra with code examples](https://hadrienj.github.io/tags/#linear-algebra)
- [Linear Algebra Class(MIT Opencourseware)](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/)
- [FastAI: Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra/blob/master/README.md)

### **Articles+**
- [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)
- [Awesome NLP](https://github.com/keon/awesome-nlp)

### **Blogs**:
* [Distill](https://distill.pub/)
* [OpenAI](https://openai.com/blog/microscope/)
* [StichFix](https://multithreaded.stitchfix.com/blog/)
* [The Gradient](https://thegradient.pub/)
